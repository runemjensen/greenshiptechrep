\section{Related work and Discussion}\label{sec:related}
\paragraph{Original method and additions}
The presented, basic method for eliminating variables from a system of linear inequalities was provided by Fourier in the 1820s \cite{fourier1}, %\cite{fourier1}}, 
while Motzkin later reintroduced the method in his Ph.D. thesis \cite{motzkin}. \v{C}ernikov later augmented Fourier's algorithm with computational simple rules, which were later rediscovered by Kohler \cite{kohler67}, to both prevent additions of (some) redundant inequalities and remove others after construction \cite{chernikov63}, and used together with these rules, the presented method is also referred to as the Fourier-\v{C}ernikov method. 
These criteria-based rules rely on keeping track of an index set for each inequality $c\in S$, which contains (the index of) the inequalities from the original system that during the procedure have been combined to obtain $c$.   
However, though the proposed rules improve the method, they are incomplete,  i.e. they do not detect or remove all redundancies {(see e.g. \cite{imbert93} or \cite{lukatskii08}).

The method has subsequently been thoroughly studied, e.g. by Kohler \cite{kohler67}, Duffin \cite{duffin74} and Imbert \cite{imbert90}, \cite{imbert93}, and several additional rules were proposed in these papers to avoid the addition of redundant inequalities. 
Improving the run time of the procedure in various ways has also been studied, for example in \cite{huynh92} and in \cite{bastrakov15}, where a faster method is proposed to check the slower of the two \v{C}ernikov criteria. 

However, as with the rules of \v{C}ernikov and Kohler, the rules suggested by Duffin and Imbert do not identify and remove all redundant inequalities from an inequality system  and it is not clear to which extend applying these methods are compatible with removing every redundant inequality by solving a linear programming problem; the \v{C}ernikoc-rules are sound, but combining them with other sound redundancy removals (even such as removing duplicates) are in general unsound (\cite{jaffar93}, \cite{huynh92}, \cite{imbert93}, \cite{fouilhe}). \cite{imbert93} further states that he does not know ``a method which suppresses all redundancies, compatible with one of [the] Fourier elimination methods''. {However, \emph{strictly redundant} inequalities, whose corresponding hyperplane does not intersect the feasible area of the system
can be removed without compromising the soundness of the procedure with the additional rules {\cite{jaffar92}}. 

Furthermore, these rules for avoiding additions of unnecessary inequalities will not be correct when our ``almost redundant''-criteria is used for removing inequalities, and therefore we have chosen not to use the mentioned rules in our implementation. However, it would be possible to use them, without the added, full redundancy check, until the system e.g. reaches a certain size, at which it will then be fully reduced and the ``index sets'' reset, such that each inequality in this system is considered original.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Redundancy removal}
Separately, particularly within linear programming, work has also been done in the area of classifying and removing redundant inequalities in a inequality system as well as finding implicit equalities, e.g. \cite{telgen83}, \cite{lassez93}, \cite{karwan83}, \cite{andersen95}, \cite{mattheiss73} to name a few. 
Many of these (e.g. \cite{telgen83} and the {majority} of the methods presented in \cite{karwan83}) are to be performed within the simplex procedure (used to optimize an LP) or use the objective function and/or the optimal extreme point for the LP, and are hence not applicable for us. On the other hand, we have used cheap redundancy-identifications e.g. described in \cite{andersen95}, \cite{brearley75} and \cite{maros} in our preprocessing and clean-up method, as well as the removal of linearly dependent inequalities from \cite{lassez93}.

Although it would be an advantage to be able to identify implicit equalities (inequalities that must hold as equalities in the feasible area) such that they can be used in Gauss-elimination, a prior implementation suggested that not much was gained when trying to identify them, while the (naïve) implementation was too time consuming.
However, it is not impossible, that our implementation could benefit from an implementation of more sophisticated methods for redundancy detection and removal instead of or in addition to (prior to) the reasonable straight-forward method applied here.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Methods for projection not based on Fourier-Motzkin-elimination}
Other methods exist for computing the projection of a feasible area of an (in)equality system, that are not based on the method by Fourier and Motzkin. 

For example, in \cite{huynh92}, the authors describe a method (based on a method from \cite{lassez90}) that they recommend for dense systems, called the extreme point method. 
The method works by finding the extreme points of the polytope $P$ defined by the convex combinations of constraints in $S$ that eliminate the variables in $Y$. 
{Since the method finds extreme points and hence inequalities in the projection space incrementally, the method can be used to approximate the projection. The method is consequently used as a supplement to Fourier-Motzkin-elimination, Gauss elimination and full redundancy removal in \cite{simon05}, when the system being projected becomes too dense and an approximation is required.}

Huynh, Lassez and Lassez \cite{huynh92} further describe a method (based on \cite{lassezlassez}), the convex hull method, in which the projection of an inequality system $S$ is computed by successive refinements of an initial approximation of the projection. 
{According to the authors, the complexity of their algorithm ``depends essentially on the dimension of the projection of the output not the size of the input'' \cite{huynh92}}, and could therefore be an interesting alternative to Fourier-Motzkin-elimination in a case like the one we consider.

Another example is the method introduced in \cite{jones04}, called equality set projection, which computes all facets of the projection by first finding a random facet and then iteratively computing all adjacent facets (without revisiting them) {using a face-lattice}. 
This method is recommended by the authors for polytopes with a low facet count and a high vertex count.
 
Yet another approach to projection is presented in the work by \cite{jones08} %\red{\cite{howe12}} 
and \cite{fouilhe}. This work is based on parametric linear programming and the possibility to formulate a projection problem as a parametric programming problem. % \blue{(in more than one way)}. 
Hence techniques for finding the solution to the latter (e.g. a ``parametric'' version of simplex) can be used to find the projection. 
%%%\cite{jones08}:``Current projection methods that can operate in general dimensions can be grouped into four classes: Fourier elimination, block elimination, vertex enumeration and gift-wrapping approaches. Fourier-Motzkin elimination was originally described by Fourier in 1824 and can be thought of as the analogue of Gaussian elimination for linear %inequalities. Several versions and improvements to Fourier’s method have been proposed ([12, 13] to name a few), although the primary contribution was due to ? Cernikov [14] in 1963. In block elimination a polyhedron called the projection cone is defined whose extreme rays can then be used to find the defining halfspaces of the projection [15]. While there exist efficient methods for computing these extreme rays, (e.g. [16–18]), this approach may generate a large, and possibly exponential, number of redundant inequalities. It is also possible to enumerate the vertices of the polytope, compute their projection and then calculate the convex hull of the result. This approach can be efficient if there is a very small number of vertices, although it is possible that there may be an exponential number of vertices. The final approach enumerates the facets of the projection directly using a gift-wrapping approach [19, 20] and has been shown to be very efficient for a large class of polyhedra.''

It is an interesting direction for future research to see, if these methods are better than the currently used method for solving the problem described in this report, or whether they can be combined advantageously with our method. The described decomposition from Section~\ref{sec:decomp} could for example still be used while each of the subproblems could be projected using any method for projection. 

\paragraph{Fourier-Motzkin-based frameworks for projection}
In \cite{simon05} Simon and King combine Fourier-Motzkin-elimination, Gauss-elimination, removal of linearly dependent inequalities and complete redundancy removal in a similar fashion as we have done, to project sparse systems, and they use the extreme point method of Huynh \textit{et al.} \cite{huynh92} to make approximations of the projection when this is necessary. Their method is implemented as part of an argument-size analyzer for logic programs and tested on a variety of these. Their elimination procedure is therefore not applied once, but instead multiple times during an analysis, and their method and results are therefore hard to compare to ours. The (in)equality systems operated on are rather sparse and quite small, %(less than $30$ (in)equalities and less than $20$ variables).
while their objective is to do the analysis more efficiently (faster) than other methods (when using the polyhedral abstract domain for program analysis). %, and on the programs they analyze, they obtain run times mainly less than a few seconds.

Lukatskii and Shapot \cite{lukatskii08}, \cite{shapot12} describe and implement a projection method using Fourier-Motzkin-elimination augmented with \v{C}ernikov's rules. They further use a techniques for full redundancy removal examining the solution matrix for a basic solution. %\blue{(a solution, where at least $n$ out of $m$ inequalities are turned into equalities for a system with $n$ variables)}. 
Further they present and apply a method for ``additional matrix clean up'' where some almost redundant inequalities are removed. Their method for this is a little more elaborate than ours and involves a successive increase of the allowable deviation (corresponding to our $\epsilon$) and a permissible maximal ratio between the number of inequalities in the current system compared to the original system. 
They perform tests on a prototype implementation, where the sized of the test ranges between $81$ inequalities and $40$ variables to $201$ inequalities and $100$ variables, i.e. the systems they project are much smaller than the ones we have considered; granted, their run time is of course also smaller than ours. 

It is shown in \cite{shapot12} that if the polyhedron is solid, then the projection algorithm is stable, while 
for a singular polyhedron, small perturbations can lead to significant changes in the projection. However, no precise definitions of solid or singular polyhedra or of a stable algorithm are given, and it is not apparent which elements (\v{C}ernikov rules, redundancy removal, additional matrix clean-up), the algorithm referred to in the proposition, contains. %I assume that stable means that the algorithm “avoids magnifying small errors'' and that a solid polyhedron is an affine translation of a full-dimensional subspace while a singular polyhedron is not.] 
Given the examples shown in e.g. \cite{jaffar93}, that \v{C}ernikov's rules together with other forms of redundancy removals are unsound in general, and since Lukatskii and Shapot exactly augment the Fourier-\v{C}ernikov method with a procedure for removing all redundant inequalities and further use ``additional matrix clean-up'' it is unclear to the authors of this report whether their presented method works in general, especially since this question is not raised or relevant references mentioned.
